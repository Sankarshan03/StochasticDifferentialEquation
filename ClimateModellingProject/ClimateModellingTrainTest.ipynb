{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install cartopy torch-geometric","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport xarray as xr\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.data import Data, Dataset\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom scipy.stats import pearsonr\n\n# Memory optimization\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# SphericalDataset class\nclass SphericalDataset(Dataset):\n    def __init__(self, file_path, transform=None, normalize=True):\n        self.transform = transform\n        print(f\"Loading data from {file_path}\")\n        self.ds = xr.open_dataset(file_path)\n        self.lats = self.ds.lat.values\n        self.lons = self.ds.lon.values\n        self.times = self.ds.time.values\n        self.tas = self.ds.tas.values\n        self.nlat = len(self.lats)\n        self.nlon = len(self.lons)\n        lon_grid, lat_grid = np.meshgrid(self.lons, self.lats)\n        self.grid_points = np.stack([lon_grid.flatten(), lat_grid.flatten()], axis=1)\n        self.edge_index = self._create_graph_connections()\n        if normalize:\n            self._normalize_features()\n        print(f\"Dataset created with {len(self.times)} timesteps, grid size: {self.nlat}x{self.nlon}\")\n    \n    def _normalize_features(self):\n        data_reshaped = self.tas.reshape(len(self.times), -1)\n        self.scaler = StandardScaler()\n        self.scaler.fit(data_reshaped)\n        normalized_data = self.scaler.transform(data_reshaped)\n        self.tas = normalized_data.reshape(self.tas.shape)\n        print(\"Data normalized\")\n    \n    def _create_graph_connections(self):\n        edges = []\n        for i in range(self.nlat):\n            for j in range(self.nlon):\n                node_idx = i * self.nlon + j\n                neighbors = []\n                if i > 0:\n                    neighbors.append((i-1) * self.nlon + j)\n                if i < self.nlat - 1:\n                    neighbors.append((i+1) * self.nlon + j)\n                west_j = (j - 1) % self.nlon\n                neighbors.append(i * self.nlon + west_j)\n                east_j = (j + 1) % self.nlon\n                neighbors.append(i * self.nlon + east_j)\n                for neighbor in neighbors:\n                    edges.append([node_idx, neighbor])\n        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n        return edge_index\n    \n    def __len__(self):\n        return len(self.times) - 1\n    \n    def __getitem__(self, idx):\n        x = self.tas[idx].reshape(-1, 1).astype(np.float32)\n        y = self.tas[idx + 1].reshape(-1, 1).astype(np.float32)\n        pos = torch.tensor(self.grid_points, dtype=torch.float)\n        data = Data(x=torch.tensor(x, dtype=torch.float), y=torch.tensor(y, dtype=torch.float),\n                    edge_index=self.edge_index, pos=pos)\n        if self.transform:\n            data = self.transform(data)\n        return data\n\n# SphericalGNN class\nclass SphericalGNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, heads=4, dropout=0.1):\n        super(SphericalGNN, self).__init__()\n        self.num_layers = num_layers\n        self.embedding = nn.Linear(input_dim, hidden_dim)\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n        self.convs.append(GATConv(hidden_dim, hidden_dim, heads=heads, dropout=dropout))\n        self.batch_norms.append(nn.BatchNorm1d(hidden_dim * heads))\n        for _ in range(num_layers - 2):\n            self.convs.append(GATConv(hidden_dim * heads, hidden_dim, heads=heads, dropout=dropout))\n            self.batch_norms.append(nn.BatchNorm1d(hidden_dim * heads))\n        self.convs.append(GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=dropout))\n        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n        self.output_layer = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.act = nn.ELU()\n    \n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.embedding(x)\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = self.act(x)\n            x = self.dropout(x)\n        x = self.output_layer(x)\n        return x\n\n# Evaluation function (fixed)\ndef memory_efficient_evaluate(model, test_loader, criterion, dataset, device='cuda'):\n    model.eval()\n    test_losses = []\n    all_predictions = []\n    all_targets = []\n    \n    test_pbar = tqdm(test_loader, desc='Evaluating')\n    \n    with torch.no_grad():\n        for batch in test_pbar:\n            batch = batch.to(device)\n            outputs = model(batch)\n            loss = criterion(outputs, batch.y)\n            all_predictions.append(outputs.cpu().numpy())\n            all_targets.append(batch.y.cpu().numpy())\n            test_losses.append(loss.item())\n            test_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n            del outputs, loss, batch\n            torch.cuda.empty_cache()\n    \n    # Concatenate and reshape to (n_samples, n_nodes)\n    predictions = np.concatenate(all_predictions)  # Shape: (n_samples, n_nodes, 1)\n    targets = np.concatenate(all_targets)  # Shape: (n_samples, n_nodes, 1)\n    predictions = predictions.reshape(-1, dataset.nlat * dataset.nlon)  # Shape: (n_samples, 192*288)\n    targets = targets.reshape(-1, dataset.nlat * dataset.nlon)\n    \n    # Inverse-transform to Kelvin\n    predictions = dataset.scaler.inverse_transform(predictions)\n    targets = dataset.scaler.inverse_transform(targets)\n    \n    # Grid information\n    nlat, nlon = dataset.nlat, dataset.nlon\n    lats = dataset.lats\n    lons = dataset.lons\n    weights = np.cos(np.deg2rad(lats))[:, np.newaxis] * np.ones((1, nlon))\n    weights = weights.flatten()\n    \n    # Standard metrics\n    avg_test_loss = np.mean(test_losses)\n    mse = np.mean((predictions - targets) ** 2)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(predictions - targets))\n    \n    # Weighted MAE\n    wmae = np.sum(weights * np.abs(predictions - targets), axis=1) / np.sum(weights)\n    wmae = np.mean(wmae)\n    \n    # Spatial Correlation Coefficient (SCC)\n    scc = [pearsonr(predictions[t], targets[t])[0] for t in range(predictions.shape[0])]\n    avg_scc = np.mean(scc)\n    \n    # Temporal Correlation Coefficient (TCC)\n    tcc = [pearsonr(predictions[:, i], targets[:, i])[0] for i in range(predictions.shape[1]) if not np.isnan(pearsonr(predictions[:, i], targets[:, i])[0])]\n    avg_tcc = np.mean(tcc)\n    \n    # Anomaly Correlation Coefficient (ACC)\n    anomalies_true = targets - np.mean(targets, axis=0, keepdims=True)\n    anomalies_pred = predictions - np.mean(predictions, axis=0, keepdims=True)\n    acc_num = np.sum(anomalies_true * anomalies_pred, axis=1)\n    acc_den = np.sqrt(np.sum(anomalies_true**2, axis=1) * np.sum(anomalies_pred**2, axis=1))\n    acc = np.mean(acc_num / acc_den, where=acc_den > 0)\n    \n    # RMSE of Gradients (latitude direction)\n    grad_true = np.diff(targets, axis=1)\n    grad_pred = np.diff(predictions, axis=1)\n    rmse_grad = np.sqrt(np.mean((grad_true - grad_pred)**2))\n    \n    # Energy Conservation Error\n    energy_true = np.sum(weights * targets, axis=1)\n    energy_pred = np.sum(weights * predictions, axis=1)\n    ece = np.mean(np.abs(energy_true - energy_pred) / np.abs(energy_true))\n    \n    # Reshape for spatial maps\n    abs_errors = np.mean(np.abs(predictions - targets), axis=0).reshape(nlat, nlon)\n    tcc_map = np.array([pearsonr(predictions[:, i], targets[:, i])[0]\n                        for i in range(predictions.shape[1])]).reshape(nlat, nlon)\n    tcc_map = np.where(np.isnan(tcc_map), 0, tcc_map)\n    \n    print(f'Test Loss: {avg_test_loss:.4f}')\n    print(f'MSE: {mse:.4f} KÂ², RMSE: {rmse:.4f} K, MAE: {mae:.4f} K')\n    print(f'Weighted MAE: {wmae:.4f} K')\n    print(f'Average Spatial Correlation: {avg_scc:.4f}')\n    print(f'Average Temporal Correlation: {avg_tcc:.4f}')\n    print(f'Anomaly Correlation: {acc:.4f}')\n    print(f'RMSE of Gradients: {rmse_grad:.4f} K/node')\n    print(f'Energy Conservation Error: {ece:.4f}')\n    \n    results = {\n        'test_loss': avg_test_loss,\n        'mse': mse,\n        'rmse': rmse,\n        'mae': mae,\n        'wmae': wmae,\n        'avg_scc': avg_scc,\n        'avg_tcc': avg_tcc,\n        'acc': acc,\n        'rmse_grad': rmse_grad,\n        'ece': ece,\n        'abs_errors': abs_errors,\n        'tcc_map': tcc_map,\n        'predictions': predictions,\n        'targets': targets,\n        'lats': lats,\n        'lons': lons\n    }\n    \n    return results\n\n# Visualization function\ndef plot_evaluation_results(results, output_dir='/kaggle/working/plots'):\n    os.makedirs(output_dir, exist_ok=True)\n    \n    lats = results['lats']\n    lons = results['lons']\n    abs_errors = results['abs_errors']\n    tcc_map = results['tcc_map']\n    predictions = results['predictions']\n    targets = results['targets']\n    \n    projection = ccrs.PlateCarree()\n    \n    plt.figure(figsize=(12, 6))\n    ax = plt.axes(projection=projection)\n    ax.add_feature(cfeature.COASTLINE)\n    ax.add_feature(cfeature.BORDERS, linestyle=':')\n    c = ax.pcolormesh(lons, lats, abs_errors, transform=projection, cmap='Reds', vmin=0)\n    plt.colorbar(c, label='Mean Absolute Error (K)')\n    ax.set_title('Spatial Distribution of Prediction Errors')\n    plt.savefig(os.path.join(output_dir, 'error_map.png'), bbox_inches='tight', dpi=300)\n    plt.close()\n    \n    plt.figure(figsize=(12, 6))\n    ax = plt.axes(projection=projection)\n    ax.add_feature(cfeature.COASTLINE)\n    ax.add_feature(cfeature.BORDERS, linestyle=':')\n    c = ax.pcolormesh(lons, lats, tcc_map, transform=projection, cmap='viridis', vmin=-1, vmax=1)\n    plt.colorbar(c, label='Temporal Correlation Coefficient')\n    ax.set_title('Temporal Correlation of Predictions')\n    plt.savefig(os.path.join(output_dir, 'tcc_map.png'), bbox_inches='tight', dpi=300)\n    plt.close()\n    \n    weights = np.cos(np.deg2rad(lats))[:, np.newaxis] * np.ones((1, len(lons)))\n    weights = weights.flatten()\n    global_mean_true = np.sum(targets * weights, axis=1) / np.sum(weights)\n    global_mean_pred = np.sum(predictions * weights, axis=1) / np.sum(weights)\n    anomalies_true = global_mean_true - np.mean(global_mean_true)\n    anomalies_pred = global_mean_pred - np.mean(global_mean_pred)\n    \n    plt.figure(figsize=(10, 4))\n    plt.plot(anomalies_true, label='True Anomalies', alpha=0.7)\n    plt.plot(anomalies_pred, label='Predicted Anomalies', alpha=0.7)\n    plt.xlabel('Timestep (Month)')\n    plt.ylabel('Global Mean Temperature Anomaly (K)')\n    plt.title('Global Temperature Anomalies (1850-2014)')\n    plt.legend()\n    plt.savefig(os.path.join(output_dir, 'anomaly_series.png'), bbox_inches='tight', dpi=300)\n    plt.close()\n    \n    errors = predictions - targets\n    plt.figure(figsize=(8, 4))\n    plt.hist(errors.flatten(), bins=50, density=True, alpha=0.7)\n    plt.xlabel('Prediction Error (K)')\n    plt.ylabel('Density')\n    plt.title('Distribution of Prediction Errors')\n    plt.savefig(os.path.join(output_dir, 'error_histogram.png'), bbox_inches='tight', dpi=300)\n    plt.close()\n\n# Main function to run evaluation\ndef run_evaluation(dataset_path, model_path, batch_size=8, test_size=0.15, device='cuda'):\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    dataset = SphericalDataset(dataset_path, normalize=True)\n    \n    num_samples = len(dataset)\n    indices = list(range(num_samples))\n    _, test_indices = train_test_split(indices, test_size=test_size, random_state=42)\n    \n    test_loader = PyGDataLoader(\n        [dataset[i] for i in test_indices],\n        batch_size=batch_size,\n        shuffle=False\n    )\n    \n    checkpoint = torch.load(model_path, map_location=device)\n    config = checkpoint.get('config', {\n        'hidden_dim': 32,\n        'num_layers': 2,\n        'attention_heads': 2,\n        'dropout': 0.2\n    })\n    \n    model = SphericalGNN(\n        input_dim=1,\n        hidden_dim=config['hidden_dim'],\n        output_dim=1,\n        num_layers=config['num_layers'],\n        heads=config['attention_heads'],\n        dropout=config['dropout']\n    ).to(device)\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    \n    criterion = nn.MSELoss()\n    \n    results = memory_efficient_evaluate(model, test_loader, criterion, dataset, device)\n    plot_evaluation_results(results)\n    \n    torch.save(results, '/kaggle/working/evaluation_results.pt')\n    \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = '/kaggle/input/cimp6-fraction/cmip6_tas_cesm2_historical.nc'\nmodel_path = '/kaggle/input/gnn-model/spherical_gnn_model.pt'\nresults = run_evaluation(dataset_path, model_path, batch_size=8, test_size=0.15, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}